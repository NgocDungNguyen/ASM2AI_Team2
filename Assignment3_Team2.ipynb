{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COSC2968|COSC3053 - Foundations of Artificial Inteligence for STEM\n",
    "# Assignment 3: Option A - Machine Learning\n",
    "# Project: Weather Prediction Model for New York City\n",
    "#### Team 2\n",
    "- Nguyen Ngoc Dung (s3978535)\n",
    "- Le Dam Quan (s4031504)\n",
    "- Nguyen Tran Ha Phan (s)\n",
    "- Phan Tri Hung (s)\n",
    "- Tran Quoc Hung (s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "This notebook demonstrates the implementation of several Machine Learning models in a pipeline to generate forecasts for the future maximum temperature in New York City based on historical weather data. The dataset comprises the following characteristics: temperature, humidity, windspeed, cloud cover, and other variables associated to weather. Our approach involves the following steps:\n",
    "- Data visualization and preprocessing \n",
    "- Feature engineering\n",
    "- Implementation of diverse machine learning models through a pipeline\n",
    "- Fine tune the models\n",
    "- Evaluation of the models\n",
    "- Provide future predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "# In[0]: IMPORT AND FUNCTIONS\n",
    "\n",
    "# Standard Libraries\n",
    "import os  # For file system operations such as creating directories.\n",
    "import warnings  # For controlling warning messages.\n",
    "from datetime import timedelta  # For handling time-related operations.\n",
    "\n",
    "# Third-party Libraries\n",
    "\n",
    "# Joblib for saving/loading models\n",
    "import joblib  # For serializing Python objects (like models).\n",
    "\n",
    "# Numpy and Pandas for numerical and data manipulation\n",
    "import numpy as np  # For numerical computations, especially with arrays.\n",
    "import pandas as pd  # For data manipulation and dataframes.\n",
    "\n",
    "# Seaborn and Matplotlib for visualizations\n",
    "import seaborn as sns  # Advanced data visualization library based on matplotlib.\n",
    "import matplotlib.pyplot as plt  # Basic plotting library.\n",
    "\n",
    "# Scipy for statistical distributions (used in RandomizedSearchCV)\n",
    "from scipy.stats import randint, uniform, loguniform  # For specifying parameter search distributions.\n",
    "\n",
    "# Sklearn utilities and transformers\n",
    "from sklearn.base import BaseEstimator, TransformerMixin  # Base classes for creating custom transformers/estimators.\n",
    "from sklearn.compose import ColumnTransformer  # For applying different preprocessing pipelines to specific columns.\n",
    "from sklearn.pipeline import Pipeline, make_pipeline  # For creating machine learning workflows.\n",
    "\n",
    "# Sklearn preprocessors and imputers\n",
    "from sklearn.preprocessing import OneHotEncoder, PolynomialFeatures, StandardScaler  # For encoding, feature generation, and scaling.\n",
    "from sklearn.impute import SimpleImputer  # For handling missing values in datasets.\n",
    "\n",
    "# Sklearn model evaluation and hyperparameter tuning\n",
    "from sklearn.model_selection import (KFold, RandomizedSearchCV, cross_val_predict, cross_val_score, train_test_split)  # For cross-validation, hyperparameter tuning, and splitting datasets.\n",
    "from sklearn.metrics import mean_squared_error  # For evaluating model performance with error metrics like MSE.\n",
    "\n",
    "# Sklearn models\n",
    "from sklearn.linear_model import BayesianRidge, Ridge, LinearRegression, Lasso  # Linear models for regression tasks.\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor  # Ensemble models for regression.\n",
    "from sklearn.svm import SVR  # Support Vector Regression.\n",
    "from sklearn.neural_network import MLPRegressor  # Neural network regressor model.\n",
    "\n",
    "# XGBoost and LightGBM - Gradient Boosting models\n",
    "from xgboost import XGBRegressor  # XGBoost regressor for gradient boosting.\n",
    "from lightgbm import LGBMRegressor  # LightGBM regressor for fast gradient boosting.\n",
    "\n",
    "# Optuna for hyperparameter optimization\n",
    "import optuna  # Framework for hyperparameter optimization.\n",
    "\n",
    "# Warnings related to model convergence\n",
    "from sklearn.exceptions import ConvergenceWarning  # To suppress warnings related to non-convergence.\n"
=======
    "import os\n",
    "import warnings\n",
    "from datetime import timedelta\n",
    "import joblib\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import randint, uniform, loguniform\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import BayesianRidge, Ridge, LinearRegression, Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import (KFold, RandomizedSearchCV, cross_val_predict,\n",
    "                                     cross_val_score, train_test_split)\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, PolynomialFeatures, StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor"
>>>>>>> ec723f1d9cb761cbde89bd7f016465d79edd035d
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress specific warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"A value is trying to be set on a copy of a DataFrame\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"No further splits with positive gain\")\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "\n",
    "# Create directories to save figures, models, and saved objects\n",
    "os.makedirs('figures', exist_ok=True)\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('saved_objects', exist_ok=True)\n",
    "\n",
    "# KFold split configuration\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Function to store models and pipelines\n",
    "def store_model_or_pipeline(obj, name=\"\"):\n",
    "    \"\"\"Store a trained model or pipeline to the 'models' folder.\"\"\"\n",
    "    if name == \"\":\n",
    "        name = type(obj).__name__\n",
    "    joblib.dump(obj, f'models/{name}.pkl')\n",
    "    print(f'{name} saved successfully.')\n",
    "\n",
    "# Function to load models or pipelines\n",
    "def load_model_or_pipeline(name):\n",
    "    \"\"\"Load a model or pipeline from the 'models' folder.\"\"\"\n",
    "    obj = joblib.load(f'models/{name}.pkl')\n",
    "    print(f'{name} loaded successfully.')\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading and Preprocessing Data\n",
    "Here, we load the dataset and perform initial preprocessing steps, such as handling missing values, removing outliers, and creating additional time-based features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('datasets/NewYork.csv')\n",
    "raw_data['datetime'] = pd.to_datetime(raw_data['datetime'])\n",
    "raw_data.drop(columns=[\"name\", \"icon\", \"stations\", \"description\"], inplace=True)\n",
    "\n",
    "# Outlier removal function\n",
    "def remove_outliers(df, column, factor=1.5):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - factor * IQR\n",
    "    upper_bound = Q3 + factor * IQR\n",
    "    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
    "\n",
    "raw_data = remove_outliers(raw_data, 'tempmax')\n",
    "\n",
    "# Handle missing values using proper assignments to avoid chained assignment warning\n",
    "for column in raw_data.columns:\n",
    "    if raw_data[column].dtype == 'object':\n",
    "        raw_data[column] = raw_data[column].fillna('Unknown')\n",
    "    else:\n",
    "        raw_data[column] = raw_data[column].fillna(raw_data[column].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Discovery\n",
    "In this section, we explore the dataset through various visualizations. We analyze the distributions of the numeric features and their correlations with the target variable (`tempmax`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n____________ Dataset info ____________')\n",
    "print(raw_data.info())              \n",
    "print('\\n____________ Some first data examples ____________')\n",
    "print(raw_data.head(3)) \n",
    "print('\\n____________ Statistics of numeric features ____________')\n",
    "print(raw_data.describe())    \n",
    "\n",
    "# Correlation matrix and clustermap for numeric features\n",
    "plt.figure(figsize=(12, 10))\n",
    "numeric_data = raw_data.select_dtypes(include=[np.number])\n",
    "corr = numeric_data.corr()\n",
    "\n",
    "# Handle any non-finite values in the correlation matrix\n",
    "if not np.isfinite(corr.values).all():\n",
    "    print(\"Warning: Correlation matrix contains non-finite values. Replacing with 0.\")\n",
    "    corr_array = np.nan_to_num(corr.values, nan=0, posinf=0, neginf=0)\n",
    "    corr = pd.DataFrame(corr_array, index=corr.index, columns=corr.columns)\n",
    "\n",
    "# Histogram of all numeric features\n",
    "numeric_features = raw_data.select_dtypes(include=[np.number]).columns\n",
    "n_features = len(numeric_features)\n",
    "n_rows = (n_features + 1) // 2\n",
    "plt.figure(figsize=(15, 5 * n_rows))\n",
    "for i, feature in enumerate(numeric_features, 1):\n",
    "    plt.subplot(n_rows, 2, i)\n",
    "    sns.histplot(raw_data[feature], kde=True)\n",
    "    plt.title(f'Distribution of {feature}')\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/hist_raw_data.png', format='png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Scatter plots of tempmax vs other numeric features\n",
    "numeric_features = [col for col in numeric_features if col != 'tempmax']\n",
    "n_features = len(numeric_features)\n",
    "n_rows = (n_features + 1) // 2\n",
    "plt.figure(figsize=(15, 5 * n_rows))\n",
    "for i, feature in enumerate(numeric_features, 1):\n",
    "    plt.subplot(n_rows, 2, i)\n",
    "    sns.scatterplot(data=raw_data, x=feature, y='tempmax', alpha=0.5)\n",
    "    plt.title(f'Max Temperature vs {feature}')\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/scatter_tempmax_vs_features.png', format='png', dpi=300)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation and Feature Engineering\n",
    "In this step, we prepare the data for model training by applying feature transformations. We create additional features based on date information, such as day of year, month, and whether the day is a weekend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedFeatureAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_features=True):\n",
    "        self.add_features = add_features\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_ = X.copy()\n",
    "        if isinstance(X_, pd.DataFrame) and self.add_features and 'datetime' in X_.columns:\n",
    "            # Add new features from datetime\n",
    "            X_['day_of_year'] = X_['datetime'].dt.dayofyear\n",
    "            X_['month'] = X_['datetime'].dt.month\n",
    "            X_['day_of_week'] = X_['datetime'].dt.dayofweek\n",
    "            X_['is_weekend'] = X_['day_of_week'].isin([5, 6]).astype(int)\n",
    "            X_['day_of_year_sin'] = np.sin(2 * np.pi * X_['day_of_year'] / 365.25)\n",
    "            X_['day_of_year_cos'] = np.cos(2 * np.pi * X_['day_of_year'] / 365.25)\n",
    "            X_.drop(columns=['datetime'], inplace=True)\n",
    "        return X_\n",
    "\n",
    "# Function to add lag features\n",
    "def add_lag_features(df, column, lags):\n",
    "    for lag in lags:\n",
    "        df[f'{column}_lag_{lag}'] = df[column].shift(lag)\n",
    "    return df\n",
    "\n",
    "# Function to add rolling averages\n",
    "def add_rolling_features(df, column, windows):\n",
    "    for window in windows:\n",
    "        df[f'{column}_rolling_{window}'] = df[column].rolling(window).mean()\n",
    "    return df\n",
    "\n",
    "# Apply lag features and rolling averages to the selected columns\n",
    "selected_columns = ['tempmax', 'tempmin', 'temp', 'feelslikemax', 'feelslikemin', 'feelslike']\n",
    "for col in selected_columns:\n",
    "    raw_data = add_lag_features(raw_data, col, lags=[1, 2, 3])\n",
    "    raw_data = add_rolling_features(raw_data, col, windows=[3, 7])\n",
    "\n",
    "# Evaluate Feature Correlation\n",
    "numeric_data = raw_data.select_dtypes(include=[np.number])\n",
    "correlation_matrix = numeric_data.corr()\n",
    "tempmax_correlation = correlation_matrix['tempmax'].sort_values(ascending=False)\n",
    "print(\"Correlation of features with tempmax:\\n\", tempmax_correlation)\n",
    "\n",
    "# Set a higher threshold to filter out less relevant features\n",
    "correlation_threshold = 0.8  # Adjust this value based on the desired strength of the correlation\n",
    "\n",
    "# Select features with high correlation (e.g., correlation > threshold or < -threshold)\n",
    "selected_features = tempmax_correlation[abs(tempmax_correlation) > correlation_threshold].index.tolist()\n",
    "selected_features.remove('tempmax')  # Remove target variable from features\n",
    "\n",
    "# List of features to exclude\n",
    "excluded_features = ['solarenergy', 'solarradiation', 'uvindex']\n",
    "\n",
    "# Remove excluded features from the selected features list\n",
    "selected_features = [feature for feature in selected_features if feature not in excluded_features]\n",
    "\n",
    "print(f\"Selected features based on correlation (threshold = {correlation_threshold}):\\n\", selected_features)\n",
    "\n",
    "# Separate numeric and categorical features\n",
    "numeric_features = [feature for feature in selected_features if raw_data[feature].dtype in [np.float64, np.int64]]\n",
    "categorical_features = [feature for feature in selected_features if raw_data[feature].dtype == 'object']\n",
    "\n",
    "\n",
    "# Define transformers for numeric and categorical features\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Define a preprocessor that applies transformations to numeric and categorical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Full pipeline with the EnhancedFeatureAdder and preprocessor\n",
    "enhanced_pipeline = Pipeline(steps=[\n",
    "    ('feature_adder', EnhancedFeatureAdder()),  # Ensure features are added here\n",
    "    ('preprocessor', preprocessor)              # Apply transformations to the features\n",
    "])\n",
    "\n",
    "# Save the full pipeline\n",
    "store_model_or_pipeline(enhanced_pipeline, name=\"full_pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training and Initial Evaluation\n",
    "In this step, we train various machine learning models on the training data. We first evaluate the models without hyperparameter tuning to establish a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the pipeline to the data\n",
    "X = raw_data.drop('tempmax', axis=1)\n",
    "y = raw_data['tempmax']\n",
    "X_processed = enhanced_pipeline.fit_transform(X, y)\n",
    "\n",
    "# Split the processed data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Models to be evaluated\n",
    "models = {\n",
    "    'RandomForestReg': RandomForestRegressor(random_state=42),\n",
    "    'GradientBoostingReg': GradientBoostingRegressor(random_state=42),\n",
    "    'LGBMReg': LGBMRegressor(random_state=42, verbosity=-1),\n",
    "    'XGBBoost': XGBRegressor(random_state=42),\n",
    "    'BayesianRidge': BayesianRidge(),\n",
    "    'LinearReg': LinearRegression(),\n",
    "    'Ridge': Ridge(random_state=42),\n",
    "    'Lasso': Lasso(random_state=42),\n",
    "    'SVR': SVR(),\n",
    "    'MLPRegressor': MLPRegressor(random_state=42),\n",
    "    'PolynomialReg': make_pipeline(PolynomialFeatures(), LinearRegression())\n",
    "}\n",
    "\n",
    "best_model_name = None\n",
    "best_cross_val_rmse = float('inf')\n",
    "\n",
    "print('\\n____________ K-Fold Cross Validation and Residual Analysis ____________')\n",
    "for name, model in models.items():\n",
    "    # Create a pipeline for each model\n",
    "    model_pipeline = Pipeline(steps=[('model', model)])\n",
    "    \n",
    "    # Perform K-Fold Cross-validation\n",
    "    cv_rmse_scores = -cross_val_score(model_pipeline, X_train, y_train, cv=kfold, scoring='neg_mean_squared_error')\n",
    "    avg_rmse = np.sqrt(cv_rmse_scores.mean())\n",
    "    \n",
    "    # Save the fold RMSE scores to saved_objects\n",
    "    joblib.dump(cv_rmse_scores, f'saved_objects/{name}_rmse.pkl')\n",
    "    \n",
    "    print(f'{name:<20} K-Fold Avg RMSE: {avg_rmse:.4f}')\n",
    "    \n",
    "    # Calculate residuals\n",
    "    y_train_pred = cross_val_predict(model_pipeline, X_train, y_train, cv=kfold)\n",
    "    residuals = y_train - y_train_pred\n",
    "    \n",
    "    # Plot residuals\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(residuals, kde=True, bins=50)\n",
    "    plt.title(f'Residuals Distribution - {name}')\n",
    "    plt.savefig(f'figures/residuals_{name}.png', format='png', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # Store the best model based on RMSE\n",
    "    if avg_rmse < best_cross_val_rmse:\n",
    "        best_cross_val_rmse = avg_rmse\n",
    "        best_model_name = name\n",
    "\n",
    "# After K-Fold Cross-Validation and residual analysis, print the final model chosen\n",
    "print(f\"\\nBest model after K-Fold Cross Validation: {best_model_name} with RMSE: {best_cross_val_rmse:.4f}\")\n",
    "\n",
    "# Assign the best model after cross-validation\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "# Store the best model after cross-validation\n",
    "store_model_or_pipeline(best_model, name=best_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-Tuning The Best Model Using RandomizedSearchCV\n",
    "Now, we fine-tune the best performing model using RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\n____________ Fine-tuning the best model: {best_model_name} ____________')\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Ensure input data is scaled\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "param_grids = {\n",
    "    'RandomForestReg': {\n",
    "        'n_estimators': randint(100, 1000),  # Reduced upper bound\n",
    "        'max_depth': randint(5, 30),  # Reduced upper bound\n",
    "        'min_samples_split': randint(2, 10),  # Reduced upper bound\n",
    "        'min_samples_leaf': randint(1, 10),  # Reduced upper bound\n",
    "        'max_features': uniform(0.3, 0.7)  # Narrowed range\n",
    "    },\n",
    "    'GradientBoostingReg': {\n",
    "        'n_estimators': randint(100, 1000),  # Reduced upper bound\n",
    "        'learning_rate': uniform(0.01, 0.1),  # Narrowed range\n",
    "        'max_depth': randint(3, 15),  # Reduced upper bound\n",
    "        'min_samples_split': randint(2, 10),  # Reduced upper bound\n",
    "        'min_samples_leaf': randint(1, 10),  # Reduced upper bound\n",
    "        'subsample': uniform(0.7, 0.3)  # Adjusted range\n",
    "    },\n",
    "    'LGBMReg': {\n",
    "        'num_leaves': randint(20, 100),  # Reduced upper bound\n",
    "        'learning_rate': uniform(0.01, 0.1),  # Narrowed range\n",
    "        'n_estimators': randint(100, 1000),  # Reduced upper bound\n",
    "        'min_child_samples': randint(1, 30),  # Reduced upper bound\n",
    "        'subsample': uniform(0.7, 0.3),  # Adjusted range\n",
    "        'colsample_bytree': uniform(0.7, 0.3)  # Adjusted range\n",
    "    },\n",
    "    'XGBBoost': {\n",
    "        'n_estimators': randint(100, 1000),  # Reduced upper bound\n",
    "        'learning_rate': uniform(0.01, 0.1),  # Narrowed range\n",
    "        'max_depth': randint(3, 15),  # Reduced upper bound\n",
    "        'min_child_weight': randint(1, 5),  # Reduced upper bound\n",
    "        'subsample': uniform(0.7, 0.3),  # Adjusted range\n",
    "        'colsample_bytree': uniform(0.7, 0.3),  # Adjusted range\n",
    "        'gamma': uniform(0, 0.3)  # Reduced upper bound\n",
    "    },\n",
    "    'BayesianRidge': {\n",
    "        'alpha_1': loguniform(1e-6, 1e-2),  # Narrowed range\n",
    "        'alpha_2': loguniform(1e-6, 1e-2),  # Narrowed range\n",
    "        'lambda_1': loguniform(1e-6, 1e-2),  # Narrowed range\n",
    "        'lambda_2': loguniform(1e-6, 1e-2)  # Narrowed range\n",
    "    },\n",
    "    'Ridge': {\n",
    "        'alpha': loguniform(1e-2, 1e2),  # Narrowed range\n",
    "        'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sag']  # Removed sparse_cg and saga\n",
    "    },\n",
    "    'Lasso': {\n",
    "        'alpha': loguniform(1e-2, 1e2)  # Narrowed range\n",
    "    },\n",
    "    'SVR': {\n",
    "        'C': loguniform(1e-2, 1e2),  # Narrowed range\n",
    "        'epsilon': loguniform(1e-3, 1),  # Reduced upper bound\n",
    "        'kernel': ['linear', 'rbf']  # Focused on most relevant kernels\n",
    "    },\n",
    "    'MLPRegressor': {\n",
    "        'hidden_layer_sizes': [(50,50), (100,50), (100,)],  # Simplified architectures\n",
    "        'activation': ['relu'],  # Focused on ReLU\n",
    "        'solver': ['adam'],  # Focused on Adam optimizer\n",
    "        'alpha': loguniform(1e-4, 1e-2),  # Narrowed range\n",
    "        'learning_rate': ['adaptive']  # Focused on adaptive learning rate\n",
    "    },\n",
    "    'PolynomialReg': {\n",
    "        'polynomialfeatures__degree': randint(2, 4),  # Reduced upper bound\n",
    "        'linearregression__fit_intercept': [True, False]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Fine-tune the best model using RandomizedSearchCV\n",
    "grid_search = RandomizedSearchCV(best_model, param_distributions=param_grids.get(best_model_name, {}),\n",
    "                                 n_iter=100, cv=kfold, \n",
    "                                 scoring='neg_mean_squared_error', n_jobs=-1, \n",
    "                                 random_state=42, verbose=1, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Save the tuned model as 'SOLUTION_model.pkl'\n",
    "best_model_tuned = grid_search.best_estimator_\n",
    "store_model_or_pipeline(best_model_tuned, name='SOLUTION_model')\n",
    "\n",
    "# Save RMSE of the tuned model\n",
    "best_tuned_rmse = np.sqrt(-grid_search.best_score_)\n",
    "joblib.dump(best_tuned_rmse, 'saved_objects/SOLUTION_model_rmse.pkl')\n",
    "print(f\"Best RMSE for {best_model_name} after tuning: {best_tuned_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze and Test The Solution\n",
    "We evaluate the best performing model on the test data to assess its generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nBest RMSE for {best_model_name} after tuning: {np.sqrt(-grid_search.best_score_):.4f}\")\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_model_tuned.predict(X_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f'\\nPerformance on test data: RMSE: {test_rmse:.4f}')\n",
    "\n",
    "# Save test RMSE results\n",
    "joblib.dump(test_rmse, 'saved_objects/test_rmse.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Future Predictions\n",
    "We use the best model to make predictions for the next 200 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the find_closest_date function\n",
    "def find_closest_date(target_date, data):\n",
    "    try:\n",
    "        target_date = target_date.replace(year=target_date.year - 1)\n",
    "    except ValueError:\n",
    "        target_date = target_date.replace(year=target_date.year - 1, day=28)\n",
    "    closest_date = data['datetime'].iloc[(data['datetime'] - target_date).abs().argsort()[0]]\n",
    "    return data.loc[data['datetime'] == closest_date].iloc[0]\n",
    "\n",
    "# Continue with the future predictions\n",
    "last_date = raw_data['datetime'].max()\n",
    "future_dates = pd.date_range(start=last_date + timedelta(days=1), periods=200)\n",
    "future_data = pd.DataFrame({'datetime': future_dates})\n",
    "\n",
    "for col in raw_data.columns:\n",
    "    if col not in ['datetime', 'tempmax']:\n",
    "        future_data[col] = future_data['datetime'].apply(lambda x: find_closest_date(x, raw_data)[col])\n",
    "\n",
    "future_processed = enhanced_pipeline.transform(future_data)\n",
    "future_pred = best_model_tuned.predict(future_processed)\n",
    "future_data['predicted_tempmax'] = future_pred\n",
    "\n",
    "# Save future predictions\n",
    "future_data[['datetime', 'predicted_tempmax']].to_csv('future_predictions_200days.csv', index=False)\n",
    "\n",
    "# Static visualization of future predictions\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(future_data['datetime'], future_data['predicted_tempmax'], label='Predicted Max Temperature', alpha=0.7)\n",
    "plt.title('Predicted Maximum Temperature for the Next 200 Days')\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Temperature (°C)')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the future prediction plot\n",
    "plt.savefig('figures/future_predictions_200days_plot.png', format='png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(\"\\nPredicted Max Temperature:\")\n",
    "print(f\"Min: {future_data['predicted_tempmax'].min():.2f}°C\")\n",
    "print(f\"Max: {future_data['predicted_tempmax'].max():.2f}°C\")\n",
    "print(f\"Avg: {future_data['predicted_tempmax'].mean():.2f}°C\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n____________ CONCLUSION ____________\")\n",
    "print(f\"\"\"\n",
    "1. Data Preprocessing:\n",
    "   - Removed outliers and handled missing values.\n",
    "   - Added engineered features: day of year, month, day of week, is_weekend.\n",
    "   - Applied scaling and one-hot encoding.\n",
    "\n",
    "2. Model Selection and Hyperparameter Tuning:\n",
    "   - Evaluated multiple models using RMSE as the sole metric.\n",
    "   - Used RandomizedSearchCV for hyperparameter optimization.\n",
    "   - The best performing model was: {best_model_name}\n",
    "\n",
    "3. Model Performance:\n",
    "   - Best model RMSE on test data: {test_rmse:.4f}\n",
    "\n",
    "4. Future Predictions:\n",
    "   - Generated predictions for the next 200 days.\n",
    "   - The predicted temperatures range from {future_data['predicted_tempmax'].min():.2f}°C to {future_data['predicted_tempmax'].max():.2f}°C.\n",
    "\n",
    "Suggestions for Further Improvement:\n",
    "1. Collect more historical data or external data sources.\n",
    "2. Experiment with more advanced time series models.\n",
    "3. Implement online learning for continuous model updates.\n",
    "4. Consider using deep learning models for complex pattern capture.\n",
    "5. Analyze prediction intervals for more robust forecasts.\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nPrediction and analysis complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Student ID | Student Name | Contribution Rate | Responsible for | Note |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| s3978535 | Nguyen Ngoc Dung | 40% | Find & research potential datasets, model fine-tuning, data exploration & feature engineering, evaluation  |   |\n",
    "| s4031504 | Le Dam Quan | 27% | Find & research potential datasets, visualization, model fine-tuning, notebook formatting and finalizing  |   |\n",
    "| s | Phan Tri Hung | 27% |   |   |\n",
    "| s | Nguyen Tran Ha Phan | 5% | Find & research datasets  |   |\n",
    "| s | Tran Quoc Hung | 1% | Find & research datasets  |   |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
