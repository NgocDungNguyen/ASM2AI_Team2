{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values before imputation:\n",
      "datetime              0\n",
      "tempmax               0\n",
      "tempmin               0\n",
      "temp                  0\n",
      "feelslikemax          0\n",
      "feelslikemin          0\n",
      "feelslike             0\n",
      "dew                   0\n",
      "humidity              0\n",
      "precip                0\n",
      "precipprob            0\n",
      "precipcover           0\n",
      "preciptype          386\n",
      "snow                  0\n",
      "snowdepth             0\n",
      "windgust              0\n",
      "windspeed             0\n",
      "winddir               0\n",
      "sealevelpressure      0\n",
      "cloudcover            0\n",
      "visibility            0\n",
      "solarradiation        0\n",
      "solarenergy           0\n",
      "uvindex               0\n",
      "severerisk           40\n",
      "sunrise               0\n",
      "sunset                0\n",
      "moonphase             0\n",
      "conditions            0\n",
      "dtype: int64\n",
      "\n",
      "Shape of data after removing outliers: (1000, 29)\n",
      "\n",
      "Missing values after imputation:\n",
      "datetime            0\n",
      "tempmax             0\n",
      "tempmin             0\n",
      "temp                0\n",
      "feelslikemax        0\n",
      "feelslikemin        0\n",
      "feelslike           0\n",
      "dew                 0\n",
      "humidity            0\n",
      "precip              0\n",
      "precipprob          0\n",
      "precipcover         0\n",
      "preciptype          0\n",
      "snow                0\n",
      "snowdepth           0\n",
      "windgust            0\n",
      "windspeed           0\n",
      "winddir             0\n",
      "sealevelpressure    0\n",
      "cloudcover          0\n",
      "visibility          0\n",
      "solarradiation      0\n",
      "solarenergy         0\n",
      "uvindex             0\n",
      "severerisk          0\n",
      "sunrise             0\n",
      "sunset              0\n",
      "moonphase           0\n",
      "conditions          0\n",
      "dtype: int64\n",
      "\n",
      "____________ Dataset info ____________\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 29 columns):\n",
      " #   Column            Non-Null Count  Dtype         \n",
      "---  ------            --------------  -----         \n",
      " 0   datetime          1000 non-null   datetime64[ns]\n",
      " 1   tempmax           1000 non-null   float64       \n",
      " 2   tempmin           1000 non-null   float64       \n",
      " 3   temp              1000 non-null   float64       \n",
      " 4   feelslikemax      1000 non-null   float64       \n",
      " 5   feelslikemin      1000 non-null   float64       \n",
      " 6   feelslike         1000 non-null   float64       \n",
      " 7   dew               1000 non-null   float64       \n",
      " 8   humidity          1000 non-null   float64       \n",
      " 9   precip            1000 non-null   float64       \n",
      " 10  precipprob        1000 non-null   int64         \n",
      " 11  precipcover       1000 non-null   float64       \n",
      " 12  preciptype        1000 non-null   object        \n",
      " 13  snow              1000 non-null   float64       \n",
      " 14  snowdepth         1000 non-null   float64       \n",
      " 15  windgust          1000 non-null   float64       \n",
      " 16  windspeed         1000 non-null   float64       \n",
      " 17  winddir           1000 non-null   float64       \n",
      " 18  sealevelpressure  1000 non-null   float64       \n",
      " 19  cloudcover        1000 non-null   float64       \n",
      " 20  visibility        1000 non-null   float64       \n",
      " 21  solarradiation    1000 non-null   float64       \n",
      " 22  solarenergy       1000 non-null   float64       \n",
      " 23  uvindex           1000 non-null   int64         \n",
      " 24  severerisk        1000 non-null   float64       \n",
      " 25  sunrise           1000 non-null   object        \n",
      " 26  sunset            1000 non-null   object        \n",
      " 27  moonphase         1000 non-null   float64       \n",
      " 28  conditions        1000 non-null   object        \n",
      "dtypes: datetime64[ns](1), float64(22), int64(2), object(4)\n",
      "memory usage: 226.7+ KB\n",
      "None\n",
      "\n",
      "____________ Some first data examples ____________\n",
      "    datetime  tempmax  tempmin  temp  feelslikemax  feelslikemin  feelslike  \\\n",
      "0 2021-12-01      9.4      4.8   7.2           9.2           1.0        5.5   \n",
      "1 2021-12-02     14.5      7.5  11.9          14.5           5.7       11.5   \n",
      "2 2021-12-03      9.2      3.6   6.7           6.9           0.9        3.6   \n",
      "\n",
      "   dew  humidity  precip  ...  cloudcover  visibility solarradiation  \\\n",
      "0 -3.4      47.4   0.000  ...        42.2        16.0           99.5   \n",
      "1  5.4      65.8   0.893  ...        68.1        15.6           48.0   \n",
      "2 -5.0      43.3   0.000  ...        16.3        16.0          106.8   \n",
      "\n",
      "   solarenergy  uvindex  severerisk              sunrise               sunset  \\\n",
      "0          8.6        5        10.0  2021-12-01T07:01:04  2021-12-01T16:29:10   \n",
      "1          4.2        3        10.0  2021-12-02T07:02:05  2021-12-02T16:28:57   \n",
      "2          9.1        5        10.0  2021-12-03T07:03:04  2021-12-03T16:28:45   \n",
      "\n",
      "   moonphase              conditions  \n",
      "0       0.91        Partially cloudy  \n",
      "1       0.95  Rain, Partially cloudy  \n",
      "2       0.98                   Clear  \n",
      "\n",
      "[3 rows x 29 columns]\n",
      "\n",
      "____________ Statistics of numeric features ____________\n",
      "                  datetime      tempmax      tempmin         temp  \\\n",
      "count                 1000  1000.000000  1000.000000  1000.000000   \n",
      "mean   2023-04-14 12:00:00    17.962500    10.794900    14.072100   \n",
      "min    2021-12-01 00:00:00    -9.300000   -14.200000   -11.600000   \n",
      "25%    2022-08-07 18:00:00    10.000000     3.800000     6.800000   \n",
      "50%    2023-04-14 12:00:00    17.800000    10.000000    13.700000   \n",
      "75%    2023-12-20 06:00:00    26.600000    18.800000    22.325000   \n",
      "max    2024-08-26 00:00:00    36.100000    26.700000    30.700000   \n",
      "std                    NaN     9.463544     8.744772     8.879228   \n",
      "\n",
      "       feelslikemax  feelslikemin    feelslike          dew     humidity  \\\n",
      "count   1000.000000   1000.000000  1000.000000  1000.000000  1000.000000   \n",
      "mean      17.809200      9.136000    13.208900     6.397600    62.516800   \n",
      "min      -15.800000    -26.000000   -20.200000   -21.200000    24.300000   \n",
      "25%        9.400000      0.775000     5.000000    -1.200000    51.900000   \n",
      "50%       17.800000     10.000000    13.650000     6.900000    61.700000   \n",
      "75%       26.525000     18.800000    22.325000    14.500000    73.525000   \n",
      "max       41.000000     29.500000    34.700000    24.000000    95.400000   \n",
      "std       10.539795     10.566489    10.257902     9.620034    14.431138   \n",
      "\n",
      "            precip  ...    windspeed      winddir  sealevelpressure  \\\n",
      "count  1000.000000  ...  1000.000000  1000.000000       1000.000000   \n",
      "mean      1.307078  ...    20.452500   190.882400       1016.448400   \n",
      "min       0.000000  ...     8.000000     0.200000        989.900000   \n",
      "25%       0.000000  ...    15.900000    75.600000       1011.900000   \n",
      "50%       0.000000  ...    19.450000   231.400000       1016.300000   \n",
      "75%       0.469000  ...    24.000000   270.125000       1021.100000   \n",
      "max      60.941000  ...    46.400000   359.800000       1037.200000   \n",
      "std       4.390026  ...     6.424748   103.137534          7.411559   \n",
      "\n",
      "        cloudcover   visibility  solarradiation  solarenergy      uvindex  \\\n",
      "count  1000.000000  1000.000000     1000.000000  1000.000000  1000.000000   \n",
      "mean     43.695200    15.045500      156.606300    13.520300     6.170000   \n",
      "min       0.100000     4.100000        0.000000     0.000000     0.000000   \n",
      "25%      14.075000    15.100000       78.075000     6.700000     4.000000   \n",
      "50%      37.150000    15.900000      148.200000    12.700000     6.000000   \n",
      "75%      72.425000    16.000000      237.725000    20.600000     9.000000   \n",
      "max     100.000000    16.000000      353.100000    30.600000    10.000000   \n",
      "std      32.501548     1.829007       93.909789     8.117261     2.962073   \n",
      "\n",
      "        severerisk    moonphase  \n",
      "count  1000.000000  1000.000000  \n",
      "mean     14.550000     0.481630  \n",
      "min      10.000000     0.000000  \n",
      "25%      10.000000     0.250000  \n",
      "50%      10.000000     0.480000  \n",
      "75%      10.000000     0.740000  \n",
      "max     100.000000     0.980000  \n",
      "std      13.308818     0.288554  \n",
      "\n",
      "[8 rows x 25 columns]\n",
      "Correlation of features with tempmax:\n",
      " tempmax                   1.000000\n",
      "feelslikemax              0.994230\n",
      "temp                      0.987500\n",
      "feelslike                 0.985032\n",
      "feelslikemin              0.957476\n",
      "tempmin                   0.957164\n",
      "tempmax_rolling_3         0.957093\n",
      "feelslikemax_rolling_3    0.954319\n",
      "temp_rolling_3            0.946263\n",
      "feelslike_rolling_3       0.945124\n",
      "feelslikemin_rolling_3    0.929811\n",
      "tempmin_rolling_3         0.929310\n",
      "tempmax_rolling_7         0.924528\n",
      "feelslikemax_rolling_7    0.922764\n",
      "temp_lag_1                0.917255\n",
      "temp_rolling_7            0.916820\n",
      "feelslike_rolling_7       0.915955\n",
      "feelslike_lag_1           0.913851\n",
      "tempmax_lag_1             0.912571\n",
      "tempmin_lag_1             0.908480\n",
      "feelslikemax_lag_1        0.907428\n",
      "feelslikemin_lag_1        0.906815\n",
      "tempmin_rolling_7         0.906555\n",
      "feelslikemin_rolling_7    0.906529\n",
      "dew                       0.898231\n",
      "temp_lag_2                0.862294\n",
      "tempmin_lag_2             0.856894\n",
      "feelslike_lag_2           0.855593\n",
      "tempmax_lag_2             0.854384\n",
      "feelslikemin_lag_2        0.850864\n",
      "feelslikemax_lag_2        0.849467\n",
      "temp_lag_3                0.841509\n",
      "tempmin_lag_3             0.838978\n",
      "feelslike_lag_3           0.833628\n",
      "feelslikemin_lag_3        0.831474\n",
      "tempmax_lag_3             0.830590\n",
      "feelslikemax_lag_3        0.824458\n",
      "solarenergy               0.578246\n",
      "solarradiation            0.577832\n",
      "uvindex                   0.541957\n",
      "severerisk                0.457812\n",
      "windgust                  0.152442\n",
      "visibility                0.122454\n",
      "humidity                  0.114140\n",
      "precipprob                0.067969\n",
      "winddir                   0.024451\n",
      "moonphase                -0.007832\n",
      "precip                   -0.061675\n",
      "precipcover              -0.123289\n",
      "snow                     -0.215264\n",
      "cloudcover               -0.218711\n",
      "snowdepth                -0.219476\n",
      "sealevelpressure         -0.238884\n",
      "windspeed                -0.256669\n",
      "Name: tempmax, dtype: float64\n",
      "Selected features based on correlation:\n",
      " ['feelslikemax', 'temp', 'feelslike', 'feelslikemin', 'tempmin', 'tempmax_rolling_3', 'feelslikemax_rolling_3', 'temp_rolling_3', 'feelslike_rolling_3', 'feelslikemin_rolling_3', 'tempmin_rolling_3', 'tempmax_rolling_7', 'feelslikemax_rolling_7', 'temp_lag_1', 'temp_rolling_7', 'feelslike_rolling_7', 'feelslike_lag_1', 'tempmax_lag_1', 'tempmin_lag_1', 'feelslikemax_lag_1', 'feelslikemin_lag_1', 'tempmin_rolling_7', 'feelslikemin_rolling_7', 'dew', 'temp_lag_2', 'tempmin_lag_2', 'feelslike_lag_2', 'tempmax_lag_2', 'feelslikemin_lag_2', 'feelslikemax_lag_2', 'temp_lag_3', 'tempmin_lag_3', 'feelslike_lag_3', 'feelslikemin_lag_3', 'tempmax_lag_3', 'feelslikemax_lag_3', 'solarenergy', 'solarradiation', 'uvindex', 'severerisk', 'windgust', 'visibility', 'humidity', 'precipcover', 'snow', 'cloudcover', 'snowdepth', 'sealevelpressure', 'windspeed']\n",
      "\n",
      "____________ K-Fold Cross Validation and Residual Analysis ____________\n",
      "RandomForestReg      K-Fold Avg RMSE: 0.6600\n",
      "GradientBoostingReg  K-Fold Avg RMSE: 0.6469\n",
      "LGBMReg              K-Fold Avg RMSE: 0.7593\n",
      "XGBBoost             K-Fold Avg RMSE: 0.7127\n",
      "BayesianRidge        K-Fold Avg RMSE: 0.0043\n",
      "LinearReg            K-Fold Avg RMSE: 0.0085\n",
      "Ridge                K-Fold Avg RMSE: 0.5029\n",
      "Lasso                K-Fold Avg RMSE: 1.3508\n",
      "SVR                  K-Fold Avg RMSE: 2.1796\n",
      "MLPRegressor         K-Fold Avg RMSE: 2.2524\n",
      "PolynomialReg        K-Fold Avg RMSE: 1.1631\n",
      "\n",
      "Best model after K-Fold Cross Validation: BayesianRidge with RMSE: 0.0043\n",
      "Model BayesianRidge saved successfully.\n",
      "\n",
      "____________ Fine-tuning the best model: BayesianRidge ____________\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Model SOLUTION_model saved successfully.\n",
      "Best RMSE for BayesianRidge after tuning: 0.0043\n",
      "\n",
      "Best RMSE for BayesianRidge after tuning: 0.0043\n",
      "\n",
      "Performance on test data: RMSE: 0.0000\n",
      "\n",
      "Predicted Max Temperature:\n",
      "Min: -2.30°C\n",
      "Max: 33.70°C\n",
      "Avg: 14.06°C\n",
      "\n",
      "____________ CONCLUSION ____________\n",
      "\n",
      "1. Data Preprocessing:\n",
      "   - Removed outliers and handled missing values.\n",
      "   - Added engineered features: day of year, month, day of week, is_weekend.\n",
      "   - Applied scaling and one-hot encoding.\n",
      "\n",
      "2. Model Selection and Hyperparameter Tuning:\n",
      "   - Evaluated multiple models using RMSE as the sole metric.\n",
      "   - Used RandomizedSearchCV for hyperparameter optimization.\n",
      "   - The best performing model was: BayesianRidge\n",
      "\n",
      "3. Model Performance:\n",
      "   - Best model RMSE on test data: 0.0000\n",
      "\n",
      "4. Future Predictions:\n",
      "   - Generated predictions for the next 200 days.\n",
      "   - The predicted temperatures range from -2.30°C to 33.70°C.\n",
      "\n",
      "Suggestions for Further Improvement:\n",
      "1. Collect more historical data or external data sources.\n",
      "2. Experiment with more advanced time series models.\n",
      "3. Implement online learning for continuous model updates.\n",
      "4. Consider using deep learning models for complex pattern capture.\n",
      "5. Analyze prediction intervals for more robust forecasts.\n",
      "\n",
      "\n",
      "Prediction and analysis complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x1500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# In[0]: IMPORT AND FUNCTIONS\n",
    "import os\n",
    "import warnings\n",
    "from datetime import timedelta\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import randint, uniform, loguniform\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import BayesianRidge, Ridge, LinearRegression, Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import (KFold, RandomizedSearchCV, cross_val_predict,\n",
    "                                     cross_val_score, train_test_split)\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, PolynomialFeatures, StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"A value is trying to be set on a copy of a DataFrame\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"No further splits with positive gain\")\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "\n",
    "# Create directories to save figures, models, and saved objects\n",
    "os.makedirs('figures', exist_ok=True)\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('saved_objects', exist_ok=True)\n",
    "\n",
    "# KFold split configuration\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Function to store models\n",
    "def store_model(model, model_name=\"\"):\n",
    "    \"\"\"Store the trained model to the 'models' folder.\"\"\"\n",
    "    if model_name == \"\":\n",
    "        model_name = type(model).__name__\n",
    "    joblib.dump(model, f'models/{model_name}_model.pkl')\n",
    "    print(f'Model {model_name} saved successfully.')\n",
    "\n",
    "# Function to load models\n",
    "def load_model(model_name):\n",
    "    \"\"\"Load a model from the 'models' folder.\"\"\"\n",
    "    model = joblib.load(f'models/{model_name}_model.pkl')\n",
    "    print(f'Model {model_name} loaded successfully.')\n",
    "    return model\n",
    "\n",
    "# In[1]: LOAD DATA AND INITIAL PREPROCESSING\n",
    "raw_data = pd.read_csv('datasets/NewYork.csv')\n",
    "raw_data['datetime'] = pd.to_datetime(raw_data['datetime'])\n",
    "raw_data.drop(columns=[\"name\", \"icon\", \"stations\", \"description\"], inplace=True)\n",
    "\n",
    "print(\"\\nMissing values before imputation:\")\n",
    "print(raw_data.isnull().sum())\n",
    "\n",
    "# Outlier removal function\n",
    "def remove_outliers(df, column, factor=1.5):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - factor * IQR\n",
    "    upper_bound = Q3 + factor * IQR\n",
    "    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
    "\n",
    "raw_data = remove_outliers(raw_data, 'tempmax')\n",
    "print(\"\\nShape of data after removing outliers:\", raw_data.shape)\n",
    "\n",
    "# Handle missing values using proper assignments to avoid chained assignment warning\n",
    "for column in raw_data.columns:\n",
    "    if raw_data[column].dtype == 'object':\n",
    "        raw_data[column] = raw_data[column].fillna('Unknown')\n",
    "    else:\n",
    "        raw_data[column] = raw_data[column].fillna(raw_data[column].median())\n",
    "\n",
    "print(\"\\nMissing values after imputation:\")\n",
    "print(raw_data.isnull().sum())\n",
    "\n",
    "# In[2]: DISCOVER THE DATA\n",
    "print('\\n____________ Dataset info ____________')\n",
    "print(raw_data.info())              \n",
    "print('\\n____________ Some first data examples ____________')\n",
    "print(raw_data.head(3)) \n",
    "print('\\n____________ Statistics of numeric features ____________')\n",
    "print(raw_data.describe())    \n",
    "\n",
    "# Correlation matrix and clustermap for numeric features\n",
    "plt.figure(figsize=(12, 10))\n",
    "numeric_data = raw_data.select_dtypes(include=[np.number])\n",
    "corr = numeric_data.corr()\n",
    "\n",
    "# Handle any non-finite values in the correlation matrix\n",
    "if not np.isfinite(corr.values).all():\n",
    "    print(\"Warning: Correlation matrix contains non-finite values. Replacing with 0.\")\n",
    "    corr_array = np.nan_to_num(corr.values, nan=0, posinf=0, neginf=0)\n",
    "    corr = pd.DataFrame(corr_array, index=corr.index, columns=corr.columns)\n",
    "\n",
    "# Create the clustermap visualization\n",
    "try:\n",
    "    sns.clustermap(corr, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "    plt.title(\"Correlation Clustermap\")\n",
    "    plt.savefig('figures/correlation_clustermap.png', format='png', dpi=300)\n",
    "except Exception as e:\n",
    "    print(f\"Error creating clustermap: {e}\")\n",
    "    print(\"Skipping clustermap creation.\")\n",
    "finally:\n",
    "    plt.close()\n",
    "\n",
    "# Histogram of all numeric features\n",
    "numeric_features = raw_data.select_dtypes(include=[np.number]).columns\n",
    "n_features = len(numeric_features)\n",
    "n_rows = (n_features + 1) // 2\n",
    "plt.figure(figsize=(15, 5 * n_rows))\n",
    "for i, feature in enumerate(numeric_features, 1):\n",
    "    plt.subplot(n_rows, 2, i)\n",
    "    sns.histplot(raw_data[feature], kde=True)\n",
    "    plt.title(f'Distribution of {feature}')\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/hist_raw_data.png', format='png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Scatter plots of tempmax vs other numeric features\n",
    "numeric_features = [col for col in numeric_features if col != 'tempmax']\n",
    "n_features = len(numeric_features)\n",
    "n_rows = (n_features + 1) // 2\n",
    "plt.figure(figsize=(15, 5 * n_rows))\n",
    "for i, feature in enumerate(numeric_features, 1):\n",
    "    plt.subplot(n_rows, 2, i)\n",
    "    sns.scatterplot(data=raw_data, x=feature, y='tempmax', alpha=0.5)\n",
    "    plt.title(f'Max Temperature vs {feature}')\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/scatter_tempmax_vs_features.png', format='png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Pairplot of main features\n",
    "main_features = ['tempmax', 'temp', 'humidity', 'windspeed', 'cloudcover', 'tempmin', 'feelslikemax', 'feelslikemin', 'feelslike']\n",
    "plt.figure(figsize=(15, 15))\n",
    "sns.pairplot(raw_data[main_features], diag_kind='kde')\n",
    "plt.suptitle(\"Pairplot of Main Features\", y=1.02)\n",
    "plt.savefig('figures/pairplot_main_features.png', format='png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# In[3]: PREPARE THE DATA \n",
    "class EnhancedFeatureAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_features=True):\n",
    "        self.add_features = add_features\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_ = X.copy()\n",
    "\n",
    "        # Ensure that the input is a DataFrame before adding features\n",
    "        if isinstance(X_, pd.DataFrame) and self.add_features and 'datetime' in X_.columns:\n",
    "            # Add new features from datetime\n",
    "            X_['day_of_year'] = X_['datetime'].dt.dayofyear\n",
    "            X_['month'] = X_['datetime'].dt.month\n",
    "            X_['day_of_week'] = X_['datetime'].dt.dayofweek\n",
    "            X_['is_weekend'] = X_['day_of_week'].isin([5, 6]).astype(int)\n",
    "            X_['day_of_year_sin'] = np.sin(2 * np.pi * X_['day_of_year'] / 365.25)\n",
    "            X_['day_of_year_cos'] = np.cos(2 * np.pi * X_['day_of_year'] / 365.25)\n",
    "            X_.drop(columns=['datetime'], inplace=True)\n",
    "        return X_\n",
    "\n",
    "# Function to add lag features\n",
    "def add_lag_features(df, column, lags):\n",
    "    for lag in lags:\n",
    "        df[f'{column}_lag_{lag}'] = df[column].shift(lag)\n",
    "    return df\n",
    "\n",
    "# Function to add rolling averages\n",
    "def add_rolling_features(df, column, windows):\n",
    "    for window in windows:\n",
    "        df[f'{column}_rolling_{window}'] = df[column].rolling(window).mean()\n",
    "    return df\n",
    "\n",
    "# Apply lag features and rolling averages to the selected columns\n",
    "selected_columns = ['tempmax', 'tempmin', 'temp', 'feelslikemax', 'feelslikemin', 'feelslike']\n",
    "for col in selected_columns:\n",
    "    raw_data = add_lag_features(raw_data, col, lags=[1, 2, 3])\n",
    "    raw_data = add_rolling_features(raw_data, col, windows=[3, 7])\n",
    "\n",
    "# Evaluate Feature Correlation\n",
    "numeric_data = raw_data.select_dtypes(include=[np.number])\n",
    "correlation_matrix = numeric_data.corr()\n",
    "tempmax_correlation = correlation_matrix['tempmax'].sort_values(ascending=False)\n",
    "print(\"Correlation of features with tempmax:\\n\", tempmax_correlation)\n",
    "\n",
    "# Select features with high correlation (e.g., correlation > 0.1 or < -0.1)\n",
    "selected_features = tempmax_correlation[abs(tempmax_correlation) > 0.1].index.tolist()\n",
    "selected_features.remove('tempmax')  # Remove target variable from features\n",
    "print(\"Selected features based on correlation:\\n\", selected_features)\n",
    "\n",
    "# Separate numeric and categorical features\n",
    "numeric_features = [feature for feature in selected_features if raw_data[feature].dtype in [np.float64, np.int64]]\n",
    "categorical_features = [feature for feature in selected_features if raw_data[feature].dtype == 'object']\n",
    "\n",
    "# Define transformers for numeric and categorical features\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Define a preprocessor that applies transformations to numeric and categorical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Full pipeline with the EnhancedFeatureAdder and preprocessor\n",
    "enhanced_pipeline = Pipeline(steps=[\n",
    "    ('feature_adder', EnhancedFeatureAdder()),  # Ensure features are added here\n",
    "    ('preprocessor', preprocessor)              # Apply transformations to the features\n",
    "])\n",
    "\n",
    "# Split features (X) and target (y)\n",
    "X = raw_data.drop('tempmax', axis=1)\n",
    "y = raw_data['tempmax']\n",
    "\n",
    "# Apply the pipeline to the data\n",
    "X_processed = enhanced_pipeline.fit_transform(X, y)\n",
    "\n",
    "# Split the processed data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Models to be evaluated\n",
    "models = {\n",
    "    'RandomForestReg': RandomForestRegressor(random_state=42),\n",
    "    'GradientBoostingReg': GradientBoostingRegressor(random_state=42),\n",
    "    'LGBMReg': LGBMRegressor(random_state=42, verbosity=-1),\n",
    "    'XGBBoost': XGBRegressor(random_state=42),\n",
    "    'BayesianRidge': BayesianRidge(),\n",
    "    'LinearReg': LinearRegression(),\n",
    "    'Ridge': Ridge(random_state=42),\n",
    "    'Lasso': Lasso(random_state=42),\n",
    "    'SVR': SVR(),\n",
    "    'MLPRegressor': MLPRegressor(random_state=42),\n",
    "    'PolynomialReg': make_pipeline(PolynomialFeatures(), LinearRegression())\n",
    "}\n",
    "\n",
    "best_model_name = None\n",
    "best_cross_val_rmse = float('inf')\n",
    "\n",
    "print('\\n____________ K-Fold Cross Validation and Residual Analysis ____________')\n",
    "for name, model in models.items():\n",
    "    # Create a pipeline for each model\n",
    "    model_pipeline = Pipeline(steps=[('model', model)])\n",
    "    \n",
    "    # Perform K-Fold Cross-validation\n",
    "    cv_rmse_scores = -cross_val_score(model_pipeline, X_train, y_train, cv=kfold, scoring='neg_mean_squared_error')\n",
    "    avg_rmse = np.sqrt(cv_rmse_scores.mean())\n",
    "    \n",
    "    # Save the fold RMSE scores to saved_objects\n",
    "    joblib.dump(cv_rmse_scores, f'saved_objects/{name}_rmse.pkl')\n",
    "    \n",
    "    print(f'{name:<20} K-Fold Avg RMSE: {avg_rmse:.4f}')\n",
    "    \n",
    "    # Calculate residuals\n",
    "    y_train_pred = cross_val_predict(model_pipeline, X_train, y_train, cv=kfold)\n",
    "    residuals = y_train - y_train_pred\n",
    "    \n",
    "    # Plot residuals\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(residuals, kde=True, bins=50)\n",
    "    plt.title(f'Residuals Distribution - {name}')\n",
    "    plt.savefig(f'figures/residuals_{name}.png', format='png', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # Store the best model based on RMSE\n",
    "    if avg_rmse < best_cross_val_rmse:\n",
    "        best_cross_val_rmse = avg_rmse\n",
    "        best_model_name = name\n",
    "\n",
    "# After K-Fold Cross-Validation and residual analysis, print the final model chosen\n",
    "print(f\"\\nBest model after K-Fold Cross Validation: {best_model_name} with RMSE: {best_cross_val_rmse:.4f}\")\n",
    "\n",
    "# Assign the best model after cross-validation\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "# Store the best model after cross-validation\n",
    "store_model(best_model, model_name=best_model_name)\n",
    "\n",
    "# In[5]: FINE-TUNE THE BEST MODEL ONLY\n",
    "print(f'\\n____________ Fine-tuning the best model: {best_model_name} ____________')\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Ensure input data is scaled\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "param_grids = {\n",
    "    'RandomForestReg': {\n",
    "        'n_estimators': randint(100, 2000),\n",
    "        'max_depth': randint(5, 50),\n",
    "        'min_samples_split': randint(2, 20),\n",
    "        'min_samples_leaf': randint(1, 20),\n",
    "        'max_features': uniform(0.1, 0.9)\n",
    "    },\n",
    "    'GradientBoostingReg': {\n",
    "        'n_estimators': randint(100, 2000),\n",
    "        'learning_rate': uniform(0.01, 0.2),\n",
    "        'max_depth': randint(3, 20),\n",
    "        'min_samples_split': randint(2, 20),\n",
    "        'min_samples_leaf': randint(1, 20),\n",
    "        'subsample': uniform(0.5, 0.5)\n",
    "    },\n",
    "    'LGBMReg': {\n",
    "        'num_leaves': randint(20, 200),\n",
    "        'learning_rate': uniform(0.01, 0.2),\n",
    "        'n_estimators': randint(100, 2000),\n",
    "        'min_child_samples': randint(1, 50),\n",
    "        'subsample': uniform(0.5, 0.5),\n",
    "        'colsample_bytree': uniform(0.5, 0.5)\n",
    "    },\n",
    "    'XGBBoost': {\n",
    "        'n_estimators': randint(100, 2000),\n",
    "        'learning_rate': uniform(0.01, 0.2),\n",
    "        'max_depth': randint(3, 20),\n",
    "        'min_child_weight': randint(1, 10),\n",
    "        'subsample': uniform(0.5, 0.5),\n",
    "        'colsample_bytree': uniform(0.5, 0.5),\n",
    "        'gamma': uniform(0, 0.5)\n",
    "    },\n",
    "    'BayesianRidge': {\n",
    "        'alpha_1': loguniform(1e-6, 1e-1),\n",
    "        'alpha_2': loguniform(1e-6, 1e-1),\n",
    "        'lambda_1': loguniform(1e-6, 1e-1),\n",
    "        'lambda_2': loguniform(1e-6, 1e-1)\n",
    "    },\n",
    "    'Ridge': {\n",
    "        'alpha': loguniform(1e-3, 1e3),\n",
    "        'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']\n",
    "    },\n",
    "    'Lasso': {\n",
    "        'alpha': loguniform(1e-3, 1e3)\n",
    "    },\n",
    "    'SVR': {\n",
    "        'C': loguniform(1e-3, 1e3),\n",
    "        'epsilon': loguniform(1e-3, 1e1),\n",
    "        'kernel': ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "    },\n",
    "    'MLPRegressor': {\n",
    "        'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],\n",
    "        'activation': ['tanh', 'relu'],\n",
    "        'solver': ['sgd', 'adam'],\n",
    "        'alpha': loguniform(1e-4, 1e-1),\n",
    "        'learning_rate': ['constant','adaptive']\n",
    "    },\n",
    "    'PolynomialReg': {\n",
    "        'polynomialfeatures__degree': randint(2, 5),\n",
    "        'linearregression__fit_intercept': [True, False]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Fine-tune the best model using RandomizedSearchCV\n",
    "grid_search = RandomizedSearchCV(best_model, param_distributions=param_grids.get(best_model_name, {}),\n",
    "                                 n_iter=100, cv=kfold, \n",
    "                                 scoring='neg_mean_squared_error', n_jobs=-1, \n",
    "                                 random_state=42, verbose=1, error_score='raise')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Save the tuned model as 'SOLUTION_model.pkl'\n",
    "best_model_tuned = grid_search.best_estimator_\n",
    "store_model(best_model_tuned, model_name='SOLUTION_model')\n",
    "\n",
    "# Save RMSE of the tuned model\n",
    "best_tuned_rmse = np.sqrt(-grid_search.best_score_)\n",
    "joblib.dump(best_tuned_rmse, 'saved_objects/SOLUTION_model_rmse.pkl')\n",
    "print(f\"Best RMSE for {best_model_name} after tuning: {best_tuned_rmse:.4f}\")\n",
    "\n",
    "# In[6]: ANALYZE AND TEST THE SOLUTION\n",
    "print(f\"\\nBest RMSE for {best_model_name} after tuning: {np.sqrt(-grid_search.best_score_):.4f}\")\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_model_tuned.predict(X_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f'\\nPerformance on test data: RMSE: {test_rmse:.4f}')\n",
    "\n",
    "# Save test RMSE results\n",
    "joblib.dump(test_rmse, 'saved_objects/test_rmse.pkl')\n",
    "\n",
    "# In[8]: MAKE FUTURE PREDICTIONS\n",
    "\n",
    "# Define the find_closest_date function\n",
    "def find_closest_date(target_date, data):\n",
    "    try:\n",
    "        target_date = target_date.replace(year=target_date.year - 1)\n",
    "    except ValueError:\n",
    "        target_date = target_date.replace(year=target_date.year - 1, day=28)\n",
    "    closest_date = data['datetime'].iloc[(data['datetime'] - target_date).abs().argsort()[0]]\n",
    "    return data.loc[data['datetime'] == closest_date].iloc[0]\n",
    "\n",
    "# Continue with the future predictions\n",
    "last_date = raw_data['datetime'].max()\n",
    "future_dates = pd.date_range(start=last_date + timedelta(days=1), periods=200)\n",
    "future_data = pd.DataFrame({'datetime': future_dates})\n",
    "\n",
    "for col in raw_data.columns:\n",
    "    if col not in ['datetime', 'tempmax']:\n",
    "        future_data[col] = future_data['datetime'].apply(lambda x: find_closest_date(x, raw_data)[col])\n",
    "\n",
    "future_processed = enhanced_pipeline.transform(future_data)\n",
    "future_pred = best_model_tuned.predict(future_processed)\n",
    "future_data['predicted_tempmax'] = future_pred\n",
    "\n",
    "# Save future predictions\n",
    "future_data[['datetime', 'predicted_tempmax']].to_csv('future_predictions_200days.csv', index=False)\n",
    "\n",
    "# Static visualization of future predictions\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(future_data['datetime'], future_data['predicted_tempmax'], label='Predicted Max Temperature', alpha=0.7)\n",
    "plt.title('Predicted Maximum Temperature for the Next 200 Days')\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Temperature (°C)')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the future prediction plot\n",
    "plt.savefig('figures/future_predictions_200days_plot.png', format='png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(\"\\nPredicted Max Temperature:\")\n",
    "print(f\"Min: {future_data['predicted_tempmax'].min():.2f}°C\")\n",
    "print(f\"Max: {future_data['predicted_tempmax'].max():.2f}°C\")\n",
    "print(f\"Avg: {future_data['predicted_tempmax'].mean():.2f}°C\")\n",
    "\n",
    "# In[9]: CONCLUSION\n",
    "print(\"\\n____________ CONCLUSION ____________\")\n",
    "print(f\"\"\"\n",
    "1. Data Preprocessing:\n",
    "   - Removed outliers and handled missing values.\n",
    "   - Added engineered features: day of year, month, day of week, is_weekend.\n",
    "   - Applied scaling and one-hot encoding.\n",
    "\n",
    "2. Model Selection and Hyperparameter Tuning:\n",
    "   - Evaluated multiple models using RMSE as the sole metric.\n",
    "   - Used RandomizedSearchCV for hyperparameter optimization.\n",
    "   - The best performing model was: {best_model_name}\n",
    "\n",
    "3. Model Performance:\n",
    "   - Best model RMSE on test data: {test_rmse:.4f}\n",
    "\n",
    "4. Future Predictions:\n",
    "   - Generated predictions for the next 200 days.\n",
    "   - The predicted temperatures range from {future_data['predicted_tempmax'].min():.2f}°C to {future_data['predicted_tempmax'].max():.2f}°C.\n",
    "\n",
    "Suggestions for Further Improvement:\n",
    "1. Collect more historical data or external data sources.\n",
    "2. Experiment with more advanced time series models.\n",
    "3. Implement online learning for continuous model updates.\n",
    "4. Consider using deep learning models for complex pattern capture.\n",
    "5. Analyze prediction intervals for more robust forecasts.\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nPrediction and analysis complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
